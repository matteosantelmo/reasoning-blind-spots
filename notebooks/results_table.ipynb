{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ccaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from inspect_ai.log import read_eval_log\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f06ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"matsant01/blind-spots-bench\", split=\"test\")\n",
    "df = pd.DataFrame(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from inspect_ai.log import read_eval_log\n",
    "from termcolor import colored, cprint\n",
    "\n",
    "root_dir = \"../outputs\"\n",
    "candidat_results = []\n",
    "\n",
    "# Load pricing data\n",
    "pricing_path = \"../data/models_pricing.csv\"\n",
    "if os.path.exists(pricing_path):\n",
    "    pricing_df = pd.read_csv(pricing_path)\n",
    "else:\n",
    "    print(\"Warning: pricing file not found\")\n",
    "    pricing_df = pd.DataFrame(columns=[\"model_name\", \"input_price\", \"output_price\"])\n",
    "\n",
    "\n",
    "for eval_dir in os.listdir(root_dir):\n",
    "    if eval_dir.startswith(\"eval_\") and os.path.isdir(os.path.join(root_dir, eval_dir)):\n",
    "        for model_dir in os.listdir(os.path.join(root_dir, eval_dir)):\n",
    "            model_path = os.path.join(root_dir, eval_dir, model_dir)\n",
    "            if not os.path.isdir(model_path): continue\n",
    "\n",
    "            eval_files = [f for f in os.listdir(model_path) if f.endswith(\".eval\")]\n",
    "            if not eval_files: continue\n",
    "            \n",
    "            log_file = os.path.join(model_path, eval_files[0])\n",
    "            \n",
    "            try:\n",
    "                log = read_eval_log(log_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {log_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Identify Solver and Grader\n",
    "            # We want to exclude grader cost. Grader is 'gemini-3-flash-preview'.\n",
    "            solver_cost = 0.0\n",
    "            solver_total_output_tokens = 0\n",
    "            \n",
    "            # The solver's model name as configured in the eval\n",
    "            solver_model_config = log.eval.model\n",
    "            \n",
    "            # Iterate usage to calculate cost\n",
    "            for model_name, usage in log.stats.model_usage.items():\n",
    "                \n",
    "                # Heuristic: If model_name contains 'gemini-3-flash-preview' and the solver is NOT that, skip it (it's the grader).\n",
    "                if \"gemini-3-flash-preview\" in model_name and \"gemini-3-flash-preview\" not in solver_model_config:\n",
    "                    continue\n",
    "                \n",
    "                # Try 1: Exact match with model_name\n",
    "                price_row = pricing_df[pricing_df['model_name'] == model_name]\n",
    "                \n",
    "                # Try 2: Cleaned name (remove prefix before first /)\n",
    "                if price_row.empty and \"/\" in model_name:\n",
    "                    cleaned = model_name.replace(model_name.split(\"/\")[0] + \"/\", \"\")\n",
    "                    price_row = pricing_df[pricing_df['model_name'] == cleaned]\n",
    "\n",
    "                # Try 3: Last part (after last /)\n",
    "                if price_row.empty:\n",
    "                    short = model_name.split(\"/\")[-1]\n",
    "                    price_row = pricing_df[pricing_df['model_name'] == short]\n",
    "                \n",
    "                if not price_row.empty:\n",
    "                    in_p = price_row.iloc[0]['input_price']\n",
    "                    out_p = price_row.iloc[0]['output_price']\n",
    "                    \n",
    "                    # Cost = Input + Output + Reasoning\n",
    "                    r_tok = getattr(usage, 'reasoning_tokens', 0) or 0\n",
    "                    \n",
    "                    # Correction for GPT-5 double counting of reasoning in output_tokens\n",
    "                    # For GPT-5 models, output_tokens already includes reasoning_tokens\n",
    "                    if \"gpt-5\" in model_name.lower():\n",
    "                        total_output_toks_for_pricing = usage.output_tokens\n",
    "                    else:\n",
    "                        total_output_toks_for_pricing = usage.output_tokens + r_tok\n",
    "                    \n",
    "                    cost = (usage.input_tokens / 1_000_000) * in_p + \\\n",
    "                           (total_output_toks_for_pricing / 1_000_000) * out_p\n",
    "                    solver_cost += cost\n",
    "                    solver_total_output_tokens += total_output_toks_for_pricing\n",
    "                else:\n",
    "                    if \"gemini-3-flash-preview\" not in model_name: # Don't warn about grader if logic failed\n",
    "                        print(f\"Warning: No pricing found for {model_name} (Solver: {solver_model_config})\")\n",
    "            \n",
    "            # Calculate correct count\n",
    "            correct_count = 0\n",
    "            if log.samples:\n",
    "                for s in log.samples:\n",
    "                    # Check for score value 1 (or True) or 'C'\n",
    "                    if s.score and (s.score.value == 'C' or s.score.value == 1 or s.score.value is True): \n",
    "                        correct_count += 1\n",
    "            \n",
    "            n_samples = len(log.samples) if log.samples else 0\n",
    "            \n",
    "            candidat_results.append({\n",
    "                \"Model\": solver_model_config,\n",
    "                \"Correct\": correct_count,\n",
    "                \"n_samples\": n_samples,\n",
    "                \"Total Cost\": solver_cost,\n",
    "                \"Total Output Tokens\": solver_total_output_tokens,\n",
    "                \"log_file\": log_file\n",
    "            })\n",
    "\n",
    "# Filter and Finalize\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "if candidat_results:\n",
    "    # Determine standard sample size (use Max)\n",
    "    max_samples = max(r['n_samples'] for r in candidat_results)\n",
    "    \n",
    "    final_results = []\n",
    "    \n",
    "    for r in candidat_results:\n",
    "        # Check against standard sample size\n",
    "        if r['n_samples'] < max_samples:\n",
    "            cprint(f\"Skipping {r['Model']} ({os.path.basename(r['log_file'])}): {r['n_samples']} samples (expected {max_samples})\", \"yellow\")\n",
    "            continue\n",
    "            \n",
    "        accuracy = r['Correct'] / r['n_samples']\n",
    "        # Calculate cost per 100 samples\n",
    "        cost_per_100_samples = (r['Total Cost'] / r['n_samples'] * 100) if r['n_samples'] > 0 else 0\n",
    "        # Calculate avg output tokens per sample (output + reasoning)\n",
    "        avg_output_tokens = r['Total Output Tokens'] / r['n_samples'] if r['n_samples'] > 0 else 0\n",
    "        \n",
    "        # Clean model name (everything after last /)\n",
    "        model_name = r['Model'].split(\"/\")[-1]\n",
    "        \n",
    "        final_results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Accuracy\": accuracy * 100, # in percentage\n",
    "                \"Cost/100 Samples ($)\": cost_per_100_samples,\n",
    "                \"Output Toks/Sample\": avg_output_tokens\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_results = pd.DataFrame(final_results)\n",
    "    if not df_results.empty:\n",
    "        # Sort\n",
    "        df_results = df_results.sort_values(by=\"Accuracy\", ascending=False)\n",
    "        \n",
    "        # --- Generate Markdown Table ---\n",
    "        # floatfmt=\".3f\" ensures 3 decimal places for all float columns\n",
    "        md_table = df_results.to_markdown(index=False, floatfmt=\".2f\")\n",
    "        \n",
    "        print(\"Generated Markdown Leaderboard:\")\n",
    "        print(md_table)\n",
    "\n",
    "        # Save Markdown to file (instead of HTML)\n",
    "        table_path = \"../data/text_only_leaderboard.md\"\n",
    "        with open(table_path, \"w\") as f:\n",
    "            f.write(md_table)\n",
    "        print(f\"Saved Markdown to '{table_path}'\")\n",
    "        \n",
    "        # --- Update README.md ---\n",
    "        readme_path = \"../README.md\"\n",
    "        if os.path.exists(readme_path):\n",
    "            with open(readme_path, \"r\") as f:\n",
    "                readme_content = f.read()\n",
    "\n",
    "            start_marker = \"<!-- LEADERBOARD-START -->\"\n",
    "            end_marker = \"<!-- LEADERBOARD-END -->\"\n",
    "            \n",
    "            if start_marker in readme_content and end_marker in readme_content:\n",
    "                start_idx = readme_content.find(start_marker) + len(start_marker)\n",
    "                end_idx = readme_content.find(end_marker)\n",
    "                \n",
    "                # Check if markers are in correct order\n",
    "                if start_idx < end_idx:\n",
    "                    # Create the new content preserving outside content\n",
    "                    # Add newlines ensuring separation\n",
    "                    new_content = readme_content[:start_idx] + \"\\n\" + md_table + \"\\n\" + readme_content[end_idx:]\n",
    "                    \n",
    "                    with open(readme_path, \"w\") as f:\n",
    "                        f.write(new_content)\n",
    "                    print(\"Updated README.md with new leaderboard table.\")\n",
    "                else:\n",
    "                    print(\"Error: README markers are malformed (start after end).\")\n",
    "            else:\n",
    "                print(\"Warning: Leaderboard markers not found in README.md.\")\n",
    "    else:\n",
    "        print(\"No valid results remaining after filtering.\")\n",
    "else:\n",
    "    print(\"No results found.\")\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a46400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471beac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
