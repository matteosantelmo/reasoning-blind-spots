{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from inspect_ai.log import read_eval_log\n",
    "\n",
    "root_dir = \"../outputs\"\n",
    "candidat_results = []\n",
    "\n",
    "# Load pricing data\n",
    "pricing_path = \"../data/models_pricing.csv\"\n",
    "pricing_df = pd.read_csv(pricing_path)\n",
    "\n",
    "pricing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_model_name(model_name: str) -> str:\n",
    "\tif \"google/\" in model_name:\n",
    "\t\treturn model_name.split(\"/\")[-1].strip()\n",
    "\telif \"openai/\" in model_name:\n",
    "\t\tif \"gpt\" in model_name or \"/o3\" in model_name:\n",
    "\t\t\treturn model_name.split(\"/\")[-1].strip()\n",
    "\t\telse:\n",
    "\t\t\treturn model_name.split(\"openai/\")[-1]\n",
    "\n",
    "\telse:\n",
    "\t\treturn model_name.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "results = []\n",
    "for eval_dir in os.listdir(root_dir):\n",
    "    if eval_dir.startswith(\"eval_\") and os.path.isdir(os.path.join(root_dir, eval_dir)):\n",
    "        \n",
    "        for model_dir in os.listdir(os.path.join(root_dir, eval_dir)):\n",
    "            model_path = os.path.join(root_dir, eval_dir, model_dir)\n",
    "            \n",
    "            try:\n",
    "                eval_files = [f for f in os.listdir(model_path) if f.endswith(\".eval\")]\n",
    "                log_file = os.path.join(model_path, eval_files[0])\n",
    "                log = read_eval_log(log_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {log_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "            n_models = len(log.stats.model_usage.keys())\n",
    "            if n_models == 1:\n",
    "                # Edge case: the same model is used for solving and grading\n",
    "                model_name = clean_model_name(list(log.stats.model_usage.keys())[0])\n",
    "\n",
    "                # Compute cost for grading only\n",
    "                gr_in_tks = 0\n",
    "                gr_out_tks = 0\n",
    "                gr_reas_tks = 0\n",
    "\n",
    "                for sample in log.samples:\n",
    "                    usage = sample.scores[\"model_graded_qa_with_reasoning_stripped\"].metadata[\"usage\"]\n",
    "                    gr_in_tks += usage[\"input_tokens\"]\n",
    "                    gr_out_tks += usage[\"output_tokens\"]\n",
    "                    gr_reas_tks += usage[\"reasoning_tokens\"] if usage[\"reasoning_tokens\"] is not None else 0\n",
    "\n",
    "                # Subtract to obtain solving usage\n",
    "                usage = log.stats.model_usage[list(log.stats.model_usage.keys())[0]]\n",
    "                in_tks = usage.input_tokens - gr_in_tks\n",
    "                out_tks = usage.output_tokens - gr_out_tks\n",
    "                reas_tks = usage.reasoning_tokens - gr_reas_tks\n",
    "                reas_tks = 0 if reas_tks is None else reas_tks\n",
    "\n",
    "                input_price_per_1M = pricing_df[pricing_df[\"model_name\"] == model_name][\"input_price\"].values[0]\n",
    "                output_price_per_1M = pricing_df[pricing_df[\"model_name\"] == model_name][\"output_price\"].values[0]\n",
    "                \n",
    "                input_cost = (in_tks / 1_000_000) * input_price_per_1M\n",
    "                output_cost = ((out_tks + reas_tks) / 1_000_000) * output_price_per_1M\n",
    "                total_cost = input_cost + output_cost\n",
    "\n",
    "            elif n_models > 1:\n",
    "                d = log.stats.model_usage.copy()\n",
    "                d.pop(\"google/gemini-3-flash-preview\", None)    # Remove grader usage\n",
    "                model_name = clean_model_name(list(d.keys())[0])\n",
    "\n",
    "                if \"gpt-5\" in model_name.lower() or \"o3\" in model_name.lower():\n",
    "                    in_tks = out_tks = reas_tks = 0\n",
    "                    for sample in log.samples:\n",
    "                        if sample.error:\n",
    "                            continue\n",
    "                        usage = sample.model_usage[list(d.keys())[0]]\n",
    "                        in_tks += usage.input_tokens\n",
    "                        out_tks += usage.output_tokens\n",
    "                else:\n",
    "                    usage = list(d.values())[0]\n",
    "                    in_tks, out_tks, reas_tks = usage.input_tokens, usage.output_tokens, usage.reasoning_tokens\n",
    "                    reas_tks = 0 if reas_tks is None else reas_tks\n",
    "\n",
    "                # Get pricing info\n",
    "                pricing_row = pricing_df[pricing_df[\"model_name\"] == model_name]\n",
    "                input_price_per_1M = pricing_row[\"input_price\"].values[0]\n",
    "                output_price_per_1M = pricing_row[\"output_price\"].values[0]\n",
    "\n",
    "                input_cost = (in_tks / 1_000_000) * input_price_per_1M\n",
    "                output_cost = ((out_tks + reas_tks) / 1_000_000) * output_price_per_1M\n",
    "                total_cost = input_cost + output_cost\n",
    "\n",
    "            else:\n",
    "                print(f\"Warning: No model usage found in {log_file}\")\n",
    "                continue\n",
    "\n",
    "            # Compute number valid samples\n",
    "            num_valid = sum(1 for sample in log.samples if sample.score is not None and sample.error is None)\n",
    "            num_correct = sum(1 for sample in log.samples if sample.score is not None and sample.score.value in [1, True, \"C\"])\n",
    "            num_total_samples = len(log.samples)\n",
    "            \n",
    "            scores_by_id = {}\n",
    "            for sample in log.samples:\n",
    "                if sample.id not in scores_by_id:\n",
    "                    scores_by_id[sample.id] = []\n",
    "                scores_by_id[sample.id].append(1 if sample.score is not None and sample.score.value in [1, True, \"C\"] else 0)\n",
    "            pass_at_2 = sum(1 for scores in scores_by_id.values() if sum(scores[:2]) >= 1) / len(scores_by_id)\n",
    "            std = np.mean([np.std(scores) for scores in scores_by_id.values()])\n",
    "\n",
    "            results.append({\n",
    "                \"eval_dir\": eval_dir,\n",
    "                \"model_name\": model_name,\n",
    "                \"accuracy\": num_correct / num_total_samples,    # accuracy over all samples (invalid counted as incorrect)\n",
    "                \"pass_at_2\": pass_at_2,\n",
    "                \"accuracy_std\": std,\n",
    "                \"num_samples\": num_total_samples,\n",
    "                \"num_valid_samples\": num_valid,\n",
    "                \"num_correct_samples\": num_correct,\n",
    "                \"input_tokens\": in_tks,\n",
    "                \"output_tokens\": out_tks,\n",
    "                \"reasoning_tokens\": reas_tks,\n",
    "                \"input_cost\": input_cost,\n",
    "                \"output_cost\": output_cost,\n",
    "                \"total_cost\": total_cost,\n",
    "                \"avg_price_per_sample\": total_cost / num_valid if num_valid > 0 else 0,  # average only over valid samples\n",
    "                \"avg_out_tokens_per_sample\": (out_tks + reas_tks) / num_valid if num_valid > 0 else 0,\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972240e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Solver's cost: ${:.2f}\".format(df[\"total_cost\"].sum()))\n",
    "print(\"\\t- openai cost: ${:.2f}\".format(df[df[\"model_name\"].str.contains(\"gpt\") | df[\"model_name\"].str.contains(\"o3\")][\"total_cost\"].sum()))\n",
    "print(\"\\t- google cost: ${:.2f}\".format(df[df[\"model_name\"].str.contains(\"gemini\")][\"total_cost\"].sum()))\n",
    "print(\"\\t- Open models: ${:.2f}\".format(df[~(df[\"model_name\"].str.contains(\"gpt\") | df[\"model_name\"].str.contains(\"o3\") | df[\"model_name\"].str.contains(\"gemini\"))][\"total_cost\"].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a0fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update README.md\n",
    "str_for_readme = \"\"\n",
    "\n",
    "for eval_dir in df['eval_dir'].unique():\n",
    "\ttmp_df = df[df['eval_dir'] == eval_dir].copy()\n",
    "\ttmp_df = tmp_df[[\"model_name\", \"accuracy\", \"pass_at_2\", \"avg_price_per_sample\", \"avg_out_tokens_per_sample\"]]\n",
    "\ttmp_df = tmp_df.sort_values(by=\"accuracy\", ascending=False)\n",
    "\ttmp_df[\"accuracy\"] = tmp_df[\"accuracy\"].apply(lambda x: f\"{100 * x:.2f}%\")\n",
    "\ttmp_df[\"pass_at_2\"] = tmp_df[\"pass_at_2\"].apply(lambda x: f\"{100 * x:.2f}%\")\n",
    "\ttmp_df[\"model_name\"] = tmp_df[\"model_name\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "\ttmp_df[\"avg_price_per_sample\"] = (tmp_df[\"avg_price_per_sample\"] * 100).apply(lambda x: f\"${x:.3f}\")\n",
    "\ttmp_df[\"avg_out_tokens_per_sample\"] = tmp_df[\"avg_out_tokens_per_sample\"].apply(lambda x: f\"{x:.1f}\")\n",
    "\ttmp_df = tmp_df.rename(columns={\n",
    "\t\t\"model_name\": \"Model\",\n",
    "\t\t\"accuracy\": \"Pass@1\",\n",
    "\t\t\"pass_at_2\": \"Pass@2\",\n",
    "\t\t\"avg_price_per_sample\": \"Price / 100 Sample\",\n",
    "\t\t\"avg_out_tokens_per_sample\": \"Output Tks / Sample\"\n",
    "\t})\n",
    "\n",
    "\tif eval_dir == \"eval_text\":\n",
    "\t\ttitle = \"üìù‚Üíüìù Text-only\"\n",
    "\telif eval_dir == \"eval_mm\":\n",
    "\t\ttitle = \"üñºÔ∏è‚Üíüìù Multimodal-to-text\"\n",
    "\telse:\n",
    "\t\ttitle = eval_dir\n",
    "\n",
    "\tsection_header = f\"#### {title} Evaluation\\n\"\n",
    "\ttable_md = tmp_df.to_markdown(index=False)\n",
    "\tstr_for_readme += section_header + table_md + \"\\n\\n\"\n",
    "\n",
    "# Open the README, find the markers and replace the content in between\n",
    "with open(\"../README.md\", \"r\") as f:\n",
    "\treadme_content = f.read()\n",
    "start_marker = \"<!-- LEADERBOARD-START -->\"\n",
    "end_marker = \"<!-- LEADERBOARD-END -->\"\n",
    "start_index = readme_content.index(start_marker) + len(start_marker)\n",
    "end_index = readme_content.index(end_marker)\n",
    "new_readme_content = (readme_content[:start_index] + \"\\n\\n\" +\n",
    "\t\t\t\t\t  str_for_readme +\n",
    "\t\t\t\t\t  readme_content[end_index:])\n",
    "with open(\"../README.md\", \"w\") as f:\n",
    "\tf.write(new_readme_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7a74b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
