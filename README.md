# Reasoning Blind Spots Benchmark

> **Reasoning Blind Spots** is a benchmark designed to stress test the reasoning capabilities of frontier AI models on tasks that are straightforward for humans but difficult for AI.
The questions are crafted to highlight the limitations of current AI systems and understand where they struggle in reasoning tasks.

This benchmark was developed as part of the _"Reasoning in AI"_ course at EPFL (MATH-700). <br>
Our codebase relies on [Inspect AI](https://inspect.aisi.org.uk) as the evaluation framework.

### To-Do List
- [ ] **Fix**: for samples with **multi-modal inputs**, make sure that visual inputs are also passed to the grader (the solver already gets them)
- [ ] **Estimate resource usages**:
    - [ ] create a small subset for testing (with current labels)
    - [ ] spin few diverse open models and get API keys for closed models
    - [ ] run eval on with this debug set and track costs
- [ ] Implement the code for **solver validation**
    - [ ] use the same solver as for evaluation, but on a set of already generated solutions
    - [ ] compare the solver output with ground truth verdicts
    - [ ] compute agreement metrics
- [ ] Implement computation of more **metrics**
- [ ] Collect a **list of models** to be evaluated and their configurations (for batch runs)


## Getting Started
### Codebase Structure
-   `conf/`: Configuration files (Hydra).
-   `data/`: Contains the dataset files.
-   `src/reasoning_blind_spots/`: Source code package.
    -   `dataset.py`: Dataset loading logic.
    -   `grader.py`: Scorer/Grader/Verifier logic.
    -  `solver.py`: Solver/Generator logic.
    -   `task.py`: Inspect AI task definition.
-   `main.py`: Entry point for running the benchmark with Inspect AI.
-   `pyproject.toml`: Project metadata and dependencies.


### Tutorial
<details>
<summary> Environment Setup </summary>
To set up the environment for this project, follow these steps:

1.  **Create a virtual environment:**
    ```bash
    python -m venv venv
    source venv/bin/activate
    ```

2.  **Install dependencies:**
    ```bash
    pip install -e ".[dev]"
    ```
    (This installs the project in editable mode along with dependencies from `pyproject.toml` and development tools)

3.  **Setup Pre-commit Hooks:**
    Initialize the pre-commit hooks to ensure code quality checks run before every commit:
    ```bash
    pre-commit install
    ```

4.  **Environment Variables:**
    Create a `.env` file in the root directory and add your API keys:
    ```
    OPENAI_API_KEY=...
    GOOGLE_API_KEY=...
    ANTHROPIC_API_KEY=...
    ```
</details>

<details>
<summary> Running the Benchmark </summary>

To **run the benchmark** using Inspect AI you can execute the [`main.py`](./main.py) script with your chosen configuration. Default configurations are provided in [`config.yaml`](./conf/config.yaml) or [`local_vllm.yaml`](./conf/local_vllm.yaml), but parameters can be overridden via command line arguments.

Example command to run the benchmark with a specific model:
```bash
python main.py \
    solver.model_name="gemini-2.0-flash-lite" \
    solver.backend="google" \
    solver.generate_config.reasoning_tokens=0
# Or, call the main script using a specific config file
python main.py --config-name local_vllm
```
</details>

<details>
<summary> Visualizing Results </summary>
Inspect AI provides a built-in dashboard that allows to visualize the results of the benchmark runs, what was generated by both the solver and the grader, and various metrics (e.g. accuracy, token counts, ...)

To **visualize the results**, you can launch Inspect AI visualization tool:
```bash
inspect view --log-dir ./outputs/<your-run-dir>/
```

</details>
