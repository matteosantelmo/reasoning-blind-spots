# Reasoning Blind Spots Benchmark

> **Reasoning Blind Spots** is a benchmark designed to stress test the reasoning capabilities of frontier AI models on tasks that are straightforward for humans but difficult for AI.
The questions are crafted to highlight the limitations of current AI systems and understand where they struggle in reasoning tasks. Download the dataset from [HuggingFace ðŸ¤—](https://huggingface.co/datasets/matsant01/blind-spots-bench).

This benchmark was developed as part of the _"Reasoning in AI"_ course at EPFL ([MATH-700](https://edu.epfl.ch/coursebook/en/reasoning-in-artificial-intelligence-MATH-700)).
Our codebase relies on [Inspect AI](https://inspect.aisi.org.uk) as the evaluation framework.

### Leaderboard
#### Text-Only
<!-- LEADERBOARD-START -->
| Model                       |   Accuracy |   Cost/100 Samples ($) |   Output Toks/Sample |
|:----------------------------|-----------:|-----------------------:|---------------------:|
| gpt-5                       |      70.18 |                   3.41 |              3396.08 |
| gpt-5-mini                  |      62.28 |                   0.43 |              2145.41 |
| gemini-2.5-pro              |      59.65 |                   3.10 |              3088.30 |
| gpt-oss-120b                |      58.77 |                   0.02 |              1656.06 |
| gemini-2.5-flash            |      49.12 |                   0.96 |              3819.24 |
| Qwen3-VL-235B-A22B-Instruct |      45.61 |                   0.11 |              1467.75 |
| Apertus-70B-Instruct-2509   |      15.79 |                   0.07 |              1151.91 |
<!-- LEADERBOARD-END -->

---

### Codebase Structure
-   `conf/`: Configuration files (Hydra).
-  `notebooks/`: Jupyter notebooks for analysis and visualization.
-  `scripts/`: Scripts for reproducing evaluations.
-  `outputs/`: Outputs from evaluations and experiments.
-   `src/reasoning_blind_spots/`: Source code package.
    -   `dataset.py`: Dataset loading logic.
    -   `grader.py`: Scorer/Grader/Verifier logic.
    -  `solver.py`: Solver/Generator logic.
    -   `task.py`: Inspect AI task definition.
-   [`main.py`](main.py): Entry point for running the benchmark with Inspect AI.
-   [`grader_validation.py`](grader_validation.py): Script to validate the grader's performance on a subset of human-labeled data.


### Development
If you want to contribute to the codebase or modify it, you can follow the instructions below to set up your development environment and run the evaluation pipeline.

<details>
<summary> Environment Setup </summary>
To set up the environment for this project, follow these steps:

1.  **Create a virtual environment:**
    ```bash
    python -m venv venv
    source venv/bin/activate
    ```

2.  **Install dependencies:**
    ```bash
    pip install -e ".[dev]"
    ```
    (This installs the project in editable mode along with dependencies from `pyproject.toml` and development tools)

3.  **Setup Pre-commit Hooks:**
    Initialize the pre-commit hooks to ensure code quality checks run before every commit:
    ```bash
    pre-commit install
    ```

4.  **Environment Variables:**
    Create a `.env` file in the root directory and add your API keys:
    ```
    OPENAI_API_KEY=...
    GOOGLE_API_KEY=...
    ANTHROPIC_API_KEY=...
    ```
</details>

<details>
<summary> Running the Benchmark </summary>

To **run the benchmark** using Inspect AI you can execute the [`main.py`](./main.py) script with your chosen configuration. Default configurations are provided in [`config.yaml`](./conf/config.yaml) or [`local_vllm.yaml`](./conf/local_vllm.yaml), but parameters can be overridden via command line arguments.

Inspect AI supports various backends (OpenAI, Anthropic, Google, local models via vLLM, etc.).
For our experiments we will mainly use Gemini/OpenAI models or open-weights models. For the latter we're using [RCP AIAAS](https://www.epfl.ch/research/facilities/rcp/ai-inference-as-a-service/) service, which hosts several open models on EPFL RCP cluster, with an API compatible with OpenAI.


Example command to run the benchmark with a specific model:
```bash
# Default config with overrides
python main.py \
    solver.model_name="gemini-2.0-flash-lite" \
    solver.backend="google" \
    solver.generate_config.reasoning_tokens=0
# Or, call the main script using a specific config file
python main.py --config-name local_vllm
```

**NOTE**, to use RCP AIAAS:
- make sure to set the right values of OPENAI_BASE_URL and OPENAI_API_KEY environment variables in your `.env` file.
- you might need to be on EPFL network (or use a VPN) to access RCP services.
</details>

<details>
<summary> Inspecting Samples and Results </summary>
Inspect AI provides a built-in dashboard that allows to visualize the results of the benchmark runs, what was generated by both the solver and the grader, and various metrics (e.g. accuracy, token counts, ...)

To **visualize the results**, you can launch Inspect AI visualization tool:
```bash
inspect view --log-dir ./outputs/<your-run-dir>/
```
Inspect AI also provides a VSCode extension that allows to visualize the results directly within VSCode.

</details>
<details>
<summary> Analysis </summary>

Using the notebooks in the `notebooks/` folder, you can analyze the results by loading the `.eval` files generated by Inspect AI runs. For example, you can use the `cost_tracking.ipynb` notebook to analyze the cost of different model runs based on token usage.
</details>

## Results
